*Project: Data Platform Apache Spark with Iceberg*

Project: Data Platform

This project sets up a data platform using Docker Compose to process streaming data and store it in Apache Iceberg format.

Prerequisites

Docker installed (https://docs.docker.com/engine/install/)
Visual Studio Code (https://code.visualstudio.com/)
Setup

Download the data_platform-group-3 folder.
Open a terminal inside the folder. You can do this by right-clicking inside the folder in your file explorer and selecting "Open in Terminal" (or equivalent option on your operating system).
Start the data platform services. In the terminal, run the following command:
Bash
docker-compose up -d
Use code with caution.
content_copy
This will start all the necessary containers in the background (detached mode).

Explanation of Services:

The docker-compose.yml file defines several services that work together:

pyspark-jupyter: Jupyter Notebook environment with Apache Spark for data processing.
ed-zookeeper: Zookeeper service for coordination.
ed-kafka: Kafka message broker for streaming data.
spark-iceberg: Apache Spark with Iceberg integration for data storage and querying.
rest: Iceberg REST API for accessing data.
minio: MinIO object storage for storing Iceberg tables.
mc: MinIO client for managing data in MinIO.
Creating a Kafka Topic

Open Docker Desktop.
Go to the "Containers" tab.
Find the "ed-kafka" container.
Click "Open shell".
Run the following command to create a Kafka topic named "device-data":
Bash
kafka-topics --create --topic device-data --bootstrap-server localhost:29092
Use code with caution.
content_copy
Accessing Jupyter Notebook

Open http://localhost:8888 in your web browser.
You may need to create a password to access Jupyter Notebook.
Running the Data Producer

Open a terminal inside Visual Studio Code.
Navigate to the project directory.
Run the following command to start the data producer:
Bash
python producer.py
Use code with caution.
content_copy
This will send data to the "device-data" Kafka topic.

Data Processing and Storage

The Apache Spark application will consume the streaming data from Kafka, process it, and save it into Iceberg tables stored in MinIO.

Additional Notes

The docker-compose.yml file contains environment variables for configuration. You can modify them to suit your needs.
Refer to the documentation of each service for more details.
This README provides a basic overview of the project. Feel free to explore the code and experiment further.